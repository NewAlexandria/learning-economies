[
  {
    "question_text": "In a dataflow diagram for a pizza ordering API, why might the database management system (DBMS) process and its data files be drawn as separate entities, even if they are within the same datacenter trust boundary?",
    "correct_answer": "To consider threats from users with direct file access separately from threats accessing the DBMS API, as these can be quite different.",
    "distractors": [
      {
        "question_text": "Because the DBMS and data files always operate under different operating system accounts.",
        "misconception": "Targets operational assumption: Student may assume a universal configuration rather than a specific modeling choice."
      },
      {
        "question_text": "To simplify the diagram by reducing the number of trust boundaries.",
        "misconception": "Targets purpose confusion: Student may think the goal is simplification, not detailed threat analysis."
      },
      {
        "question_text": "To indicate that the data files are always stored on a separate physical server from the DBMS.",
        "misconception": "Targets physical vs. logical distinction: Student may confuse logical separation for threat modeling with physical infrastructure."
      }
    ],
    "detailed_explanation": {
      "core_logic": "The text explicitly states, &#39;For the database, I&#39;ve drawn the database management system (DBMS) process separately from the actual data files. It&#39;s often useful to consider threats from users that have direct access to files separately from threats that access the DBMS API because these can be quite different.&#39; This separation allows for a more granular analysis of distinct threat vectors.",
      "distractor_analysis": "While they might operate under different OS accounts, the primary reason given is for distinct threat analysis, not a universal configuration. Separating them increases detail, not simplifies the diagram. The separation is for logical threat modeling, not necessarily a reflection of physical server deployment.",
      "analogy": "Separating the DBMS process from its data files in a diagram is like having separate security plans for the bank vault door (DBMS API) and the physical vault contents (data files), even if they&#39;re in the same building. The threats to each are distinct."
    },
    "code_snippets": [],
    "difficulty": "advanced",
    "question_type": "analysis",
    "prerequisites": [
      "THREAT_MODELING_CONCEPTS",
      "DATAFLOW_DIAGRAMS",
      "DATABASE_SECURITY_BASICS"
    ]
  },
  {
    "question_text": "Which of the following scenarios represents the most significant detection gap related to `rexec` in a typical enterprise environment?",
    "correct_answer": "A Unix server with `rexecd` enabled by default and no logging configured, despite not having an `rexec` client.",
    "distractors": [
      {
        "question_text": "A Windows NT 4 machine with an `rexec` client but no daemon.",
        "misconception": "Targets attack surface confusion: Student may focus on the presence of a client without considering the lack of a listening service."
      },
      {
        "question_text": "A Silicon Graphics IRIX machine using `rexec` for `inst` software installation.",
        "misconception": "Targets legitimate use confusion: Student may identify a known use case as a detection gap, rather than an unmonitored vulnerability."
      },
      {
        "question_text": "An internal network where `rexec` traffic is allowed to pass between internal hosts.",
        "misconception": "Targets external vs. internal risk: Student may prioritize internal traffic over the more critical external exposure or unmonitored internal service."
      }
    ],
    "detailed_explanation": {
      "core_logic": "The text highlights that &#39;almost every Unix system ships with `rexecd` enabled in `/etc/inetd.conf`&#39; and that &#39;most `rexec` daemons provide no logging whatsoever.&#39; This combination creates a significant detection gap: a widely enabled, vulnerable service that attackers can exploit without leaving a trace, especially if the organization doesn&#39;t even use `rexec` clients.",
      "distractor_analysis": "A Windows NT 4 machine with only a client and no daemon does not present a direct attack surface for `rexec` exploitation. While `rexec` is used legitimately on IRIX machines for `inst`, the primary detection gap isn&#39;t the legitimate use itself, but rather the lack of logging if that legitimate use were compromised or if `rexec` was enabled for other, unmonitored purposes. Allowing `rexec` traffic internally is a risk, but the most significant detection gap arises from the combination of default enablement and lack of logging on a service that is often not even needed.",
      "analogy": "This is like having a back door to your house that&#39;s always unlocked and has no alarm, even though you never use it. It&#39;s a much bigger detection gap than a front door you use daily but might forget to lock sometimes."
    },
    "code_snippets": [
      {
        "language": "bash",
        "code": "grep -i rexec /etc/inetd.conf",
        "context": "Command to check if `rexecd` is enabled in `inetd.conf` on a Unix-like system."
      }
    ],
    "difficulty": "advanced",
    "question_type": "analysis",
    "prerequisites": [
      "UNIX_SYSTEM_ADMINISTRATION",
      "NETWORK_SECURITY_CONCEPTS",
      "VULNERABILITY_MANAGEMENT"
    ]
  },
  {
    "question_text": "What is the relationship between an RFC number and an STD number when an RFC becomes an Internet standard?",
    "correct_answer": "The RFC keeps its original number, but it is also assigned a separate &#39;STD xxxx&#39; label, where STD numbers identify protocols and RFC numbers identify documents.",
    "distractors": [
      {
        "question_text": "The RFC number is replaced by the new STD number, as the document transitions from a request to a standard.",
        "misconception": "Targets document identification confusion: Student may assume a standard replaces the original document identifier."
      },
      {
        "question_text": "The STD number is a sub-category of the RFC number, indicating its status as a standard within the RFC series.",
        "misconception": "Targets hierarchical relationship confusion: Student may incorrectly perceive STD as a subset rather than a parallel identifier."
      },
      {
        "question_text": "Only RFCs that are &#39;Informational&#39; or &#39;Experimental&#39; can become standards, and they are then assigned an STD number.",
        "misconception": "Targets status track confusion: Student may confuse the different RFC statuses and their path to standardization."
      }
    ],
    "detailed_explanation": {
      "core_logic": "The document states, &#39;When an RFC becomes a standard, it still keeps its RFC number, but it is also given an &#39;STD xxxx&#39; label. The relationship between the STD numbers and the RFC numbers is not one-to-one. STD numbers identify protocols, whereas RFC numbers identify documents.&#39;",
      "distractor_analysis": "The first distractor is incorrect because the RFC number is retained. The second distractor misrepresents the relationship; they are distinct identifiers for different purposes. The third distractor is incorrect because &#39;Standards Track&#39; RFCs are the ones that can become standards, not &#39;Informational&#39; or &#39;Experimental&#39; ones.",
      "analogy": "Think of it like a book (RFC) that becomes a classic. It keeps its original ISBN (RFC number), but it might also be given a special &#39;Classic Edition&#39; label (STD number) to signify its new status, even though the original book is still the same."
    },
    "code_snippets": [],
    "difficulty": "advanced",
    "question_type": "definition",
    "prerequisites": [
      "NETWORK_STANDARDS"
    ]
  },
  {
    "question_text": "What detection gap arises from an organization&#39;s failure to define recommended security practices for using its automated software delivery toolchains and ensure their correct configuration?",
    "correct_answer": "Inability to detect misconfigurations or unauthorized changes within the CI/CD pipeline that could introduce vulnerabilities",
    "distractors": [
      {
        "question_text": "Lack of executive buy-in for secure development initiatives",
        "misconception": "Targets organizational vs. technical gap confusion: Student may confuse a management issue with a technical detection gap."
      },
      {
        "question_text": "Difficulty in communicating security requirements to third-party OSS providers",
        "misconception": "Targets scope confusion: Student may focus on third-party OSS rather than the internal toolchain configuration."
      },
      {
        "question_text": "Absence of segmented development, testing, and production environments",
        "misconception": "Targets environmental vs. toolchain configuration confusion: Student may confuse environment segmentation with toolchain security practices."
      }
    ],
    "detailed_explanation": {
      "core_logic": "Automated software delivery toolchains are critical for modern development, but they also represent a significant attack surface. If an organization fails to define security practices for their use and ensure correct configuration, it creates a detection gap where misconfigurations, unauthorized modifications, or malicious activities within the toolchain itself could go unnoticed. These issues could directly lead to the introduction of vulnerabilities into software or compromise the integrity of the build process, making detection of such events crucial.",
      "distractor_analysis": "Lack of executive buy-in is an organizational challenge, not a direct detection gap related to toolchain security. Difficulty communicating with third-party OSS providers is a separate supply chain security concern. The absence of segmented environments is an infrastructure-level control gap, distinct from the security practices and configuration of the toolchains operating within those environments.",
      "analogy": "Not securing your automated toolchains is like having a robot assembly line for your product but leaving the robot&#39;s programming interface unsecured and unmonitored. Any malicious change to the robot&#39;s instructions could compromise every product it builds."
    },
    "code_snippets": [],
    "difficulty": "advanced",
    "question_type": "analysis",
    "prerequisites": [
      "DEVOPS_SECURITY",
      "CI_CD_SECURITY",
      "CONFIGURATION_MANAGEMENT"
    ]
  },
  {
    "question_text": "Which two complementary properties are mandatory for a robust access control framework in distributed SDN/NFV environments to address heterogeneity and multiple enforcement points?",
    "correct_answer": "A general policy language to describe diverse resources and operations, and a compiler to translate policies into security rules for various enforcement mechanisms.",
    "distractors": [
      {
        "question_text": "Support for only static security policies and a centralized enforcement agent.",
        "misconception": "Targets requirement inversion: Student may choose the opposite of what is needed for dynamic, distributed environments."
      },
      {
        "question_text": "A simple syntax for flow filters and a single, high-level statistics dashboard.",
        "misconception": "Targets granularity and scope confusion: Student may focus on limited, SDN-specific features rather than the broader SDN/NFV requirements."
      },
      {
        "question_text": "Automated vulnerability scanning and a real-time intrusion detection system.",
        "misconception": "Targets functional confusion: Student may conflate access control framework requirements with general security tools."
      }
    ],
    "detailed_explanation": {
      "core_logic": "Due to the distributed nature and heterogeneous resources of SDN/NFV, an effective access control framework requires a general policy language capable of describing all resources and operations (VMs, VNFs, flows, instances). Additionally, a compiler is needed to translate these high-level policies into specific security rules that can be enforced across different levels and vendor technologies within the platform.",
      "distractor_analysis": "The first distractor proposes static policies and centralized enforcement, which directly contradicts the needs of dynamic, distributed SDN/NFV. The second distractor suggests a limited scope (flow filters) and a monitoring tool (dashboard) rather than core access control components. The third distractor lists general security tools, not the specific architectural components for an access control framework.",
      "analogy": "It&#39;s like needing a universal translator for a multi-lingual team (policy language) and a project manager who can assign tasks to different specialists (compiler to enforcement mechanisms)."
    },
    "code_snippets": [],
    "difficulty": "advanced",
    "question_type": "defense",
    "prerequisites": [
      "SDN_NFV_ARCHITECTURE",
      "ACCESS_CONTROL_POLICY"
    ]
  },
  {
    "question_text": "In a federated SDN coalition, if the USA detects terrorist probes and implements a rule to blacklist devices with more than three probes on illegal ports, while the UK disables external communication in specific spatial regions, how can the East-West interface enhance joint security for UAVs in a dynamically formed Coalition of the Willing (CoI)?",
    "correct_answer": "The controllers can share their respective policies, allowing UAVs from either nation to install combined security policies based on joint insights.",
    "distractors": [
      {
        "question_text": "It allows UK UAVs to directly route all their traffic through US network elements for enhanced security.",
        "misconception": "Targets control plane vs. data plane confusion: Student may confuse policy sharing with direct data routing decisions."
      },
      {
        "question_text": "It forces both nations to adopt a single, unified security policy for all UAV operations.",
        "misconception": "Targets autonomy vs. federation confusion: Student may assume federation implies complete policy unification, ignoring the &#39;partners do not fully trust each other&#39; aspect."
      },
      {
        "question_text": "It enables the US controller to directly control UK UAVs and vice versa, overriding national policies.",
        "misconception": "Targets control authority confusion: Student may assume cross-controller communication implies direct cross-national asset control, violating trust boundaries."
      }
    ],
    "detailed_explanation": {
      "core_logic": "The East-West interface enables the SDNCs of the USA and UK to share their independently developed security policies. This allows UAVs from either country, operating within a joint CoI, to benefit from the combined intelligence. For example, US UAVs can gain insights about vulnerable regions from UK policies, and UK UAVs can get rules to dynamically block devices based on US detections, even if outside their original region-based policy.",
      "distractor_analysis": "While data routing can be optimized, the primary enhancement from the East-West interface in this scenario is policy sharing for security, not direct data routing. The architecture emphasizes that &#39;partners do not fully trust each other,&#39; implying policy sharing and coordination rather than forced unification or direct overriding of national policies. Direct control of another nation&#39;s assets by a foreign controller is contrary to the federated, trust-limited nature of the coalition.",
      "analogy": "Imagine two police forces, each with unique intelligence on a criminal. The East-West interface is like a secure intelligence brief where they share their findings, allowing both forces&#39; units to act with a more complete picture, rather than one force taking over the other&#39;s operations."
    },
    "code_snippets": [],
    "difficulty": "advanced",
    "question_type": "analysis",
    "prerequisites": [
      "SDN_SECURITY",
      "COALITION_OPERATIONS",
      "POLICY_MANAGEMENT"
    ]
  },
  {
    "question_text": "What is a primary detection challenge for &#39;Immutable Architecture&#39; that could lead to a blind spot if not specifically addressed during security design?",
    "correct_answer": "Detecting unauthorized changes or tampering, as the expectation is that components should not change, making any change anomalous.",
    "distractors": [
      {
        "question_text": "Monitoring for excessive resource consumption, as immutable systems are often stateless.",
        "misconception": "Targets operational concern confusion: Student may focus on performance monitoring rather than security integrity."
      },
      {
        "question_text": "Identifying lateral movement within the network, as immutable systems are isolated.",
        "misconception": "Targets isolation assumption: Student may incorrectly assume immutability inherently provides network isolation."
      },
      {
        "question_text": "Detecting zero-day exploits in underlying operating systems, as they are frequently patched.",
        "misconception": "Targets patching confusion: Student may conflate immutability with rapid patching, which is a separate process."
      }
    ],
    "detailed_explanation": {
      "core_logic": "Immutable architecture implies that once a component is deployed, it is never modified; instead, a new, updated component replaces it. This characteristic means that any detected change or modification to a running immutable component is inherently suspicious and likely indicative of unauthorized activity or tampering. Therefore, the primary detection challenge and potential blind spot is failing to establish robust monitoring for any deviation from the expected immutable state.",
      "distractor_analysis": "While resource consumption and lateral movement are general security concerns, they are not unique detection challenges specifically arising from the &#39;immutable&#39; nature of the architecture. Immutable systems are not inherently isolated, and while they can be rapidly replaced with patched versions, this doesn&#39;t eliminate the need to detect exploits in the currently running, immutable instance.",
      "analogy": "In an immutable architecture, detecting a change is like finding a new scratch on a car that was just delivered from the factory; it shouldn&#39;t be there, and its presence immediately signals a problem."
    },
    "code_snippets": [],
    "difficulty": "advanced",
    "question_type": "defense",
    "prerequisites": [
      "CLOUD_NATIVE_SECURITY",
      "DEVOPS_SECURITY",
      "SECURITY_ARCHITECTURE"
    ]
  },
  {
    "question_text": "What is a critical detection gap that can arise from a poorly secured &#39;Infrastructure as Code&#39; (IaC) implementation, even if the deployed infrastructure itself is well-configured?",
    "correct_answer": "Lack of version control and integrity monitoring for the IaC scripts themselves, allowing malicious changes before deployment.",
    "distractors": [
      {
        "question_text": "Failure to apply security patches to the underlying cloud provider&#39;s infrastructure.",
        "misconception": "Targets shared responsibility confusion: Student may attribute cloud provider&#39;s responsibility to IaC."
      },
      {
        "question_text": "Insufficient network segmentation between different IaC-managed environments.",
        "misconception": "Targets deployment outcome confusion: Student may focus on the resulting network configuration rather than the IaC source."
      },
      {
        "question_text": "Inability to scale resources dynamically based on demand, leading to denial of service.",
        "misconception": "Targets operational benefit confusion: Student may confuse a feature of IaC with a security detection gap."
      }
    ],
    "detailed_explanation": {
      "core_logic": "Infrastructure as Code (IaC) defines and manages infrastructure through code. If the IaC scripts themselves are not properly secured with version control, access controls, and integrity monitoring, an attacker could modify these scripts. These malicious changes would then be automatically deployed, creating vulnerabilities or backdoors in the infrastructure before it even goes live, bypassing traditional runtime detection mechanisms. The detection gap is in the &#39;code&#39; phase, not just the &#39;runtime&#39; phase.",
      "distractor_analysis": "Patching the cloud provider&#39;s infrastructure is their responsibility. Network segmentation is an outcome of IaC, not a direct detection gap in the IaC process itself. Dynamic scaling is a benefit of IaC, not a security detection challenge.",
      "analogy": "A detection gap in IaC is like having a perfectly secure factory, but allowing anyone to tamper with the blueprints before the products are even built."
    },
    "code_snippets": [
      {
        "language": "bash",
        "code": "git diff origin/main -- terraform/main.tf",
        "context": "Command to review changes in an IaC file before deployment, highlighting the need for version control and review."
      }
    ],
    "difficulty": "advanced",
    "question_type": "defense",
    "prerequisites": [
      "DEVOPS_SECURITY",
      "CLOUD_SECURITY_BASICS",
      "VERSION_CONTROL_SECURITY"
    ]
  },
  {
    "question_text": "What is the primary goal of security research focused on reducing the Trusted Computing Base (TCB)?",
    "correct_answer": "To minimize the amount of code that, if compromised, could lead to a system-wide security failure.",
    "distractors": [
      {
        "question_text": "To increase the number of user programs with superuser power.",
        "misconception": "Targets TCB component confusion: Student may misunderstand the role of superuser programs within the TCB, or confuse reduction with expansion."
      },
      {
        "question_text": "To allow all I/O devices to directly affect system security without mediation.",
        "misconception": "Targets TCB scope confusion: Student may misunderstand which hardware components are typically excluded from the TCB&#39;s direct security enforcement."
      },
      {
        "question_text": "To make operating systems less efficient by forcing all components to use IPC.",
        "misconception": "Targets efficiency vs. security trade-off confusion: Student may confuse the *consequence* of some TCB reduction methods (like multiserver) with the *goal* of TCB reduction."
      }
    ],
    "detailed_explanation": {
      "core_logic": "The Trusted Computing Base (TCB) consists of the hardware and software components whose correct functioning is essential for enforcing all security rules of a system. If any part of the TCB is compromised, the entire system&#39;s security is at risk. Therefore, the primary goal of reducing the TCB is to minimize the attack surface and the complexity of the critical security-enforcing components, making it easier to verify their correctness and thus enhance overall system security. This is exemplified by systems like MINIX 3, which drastically reduce the kernel&#39;s TCB size.",
      "distractor_analysis": "Increasing superuser programs would likely *expand* the TCB, not reduce it, and introduce more points of failure. Allowing I/O devices to bypass security mediation would *weaken* security, not enhance it, and contradicts the TCB&#39;s purpose. While some TCB reduction strategies (like multiserver designs) might introduce efficiency trade-offs due to IPC, the *goal* is security enhancement, not efficiency reduction.",
      "analogy": "Reducing the TCB is like trying to secure a vault by making the vault itself as small and simple as possible, rather than trying to secure an entire building that contains the vault."
    },
    "code_snippets": [],
    "difficulty": "advanced",
    "question_type": "analysis",
    "prerequisites": [
      "TRUSTED_COMPUTING_BASE",
      "SECURITY_DESIGN_PRINCIPLES"
    ]
  },
  {
    "question_text": "Which statement accurately describes a potential drawback of OAuth 2.0&#39;s extensibility and modularity?",
    "correct_answer": "It can lead to basic incompatibility problems between implementations due to optional pieces.",
    "distractors": [
      {
        "question_text": "It forces all implementations to adhere to a single, rigid security standard, limiting innovation.",
        "misconception": "Targets flexibility misunderstanding: Student may confuse modularity with strict standardization."
      },
      {
        "question_text": "It makes clients more complex by requiring them to manage all security policies.",
        "misconception": "Targets client complexity confusion: Student may incorrectly attribute increased client complexity to modularity."
      },
      {
        "question_text": "It guarantees that any system implementing OAuth 2.0 correctly is inherently secure in practice.",
        "misconception": "Targets security assurance confusion: Student may believe spec compliance equals practical security."
      }
    ],
    "detailed_explanation": {
      "core_logic": "While OAuth 2.0&#39;s extensibility and modularity are assets, they also present a drawback: the protocol leaves many pieces optional. This flexibility can lead to incompatibility issues between different implementations, as developers might choose different optional components or interpret them differently, causing confusion and integration challenges.",
      "distractor_analysis": "The modularity of OAuth 2.0 promotes flexibility, not rigid standardization. Complexity is shifted away from clients, not onto them. The document explicitly states that implementing OAuth 2.0 correctly according to the spec does not guarantee practical security, as options can be misused or not enforced properly.",
      "analogy": "It&#39;s like building with LEGOs – you have many options and can build anything, but if two people build separate parts without coordinating, they might not fit together perfectly."
    },
    "code_snippets": [],
    "difficulty": "advanced",
    "question_type": "analysis",
    "prerequisites": [
      "OAUTH_PROTOCOL",
      "SOFTWARE_ENGINEERING_PRINCIPLES"
    ]
  },
  {
    "question_text": "What does a hacker consider a strong signal when evaluating which web application to focus their efforts on, regarding the application&#39;s security posture?",
    "correct_answer": "The quality of the application&#39;s security architecture, as it can indicate widespread vulnerabilities",
    "distractors": [
      {
        "question_text": "The number of users the application has, indicating potential impact",
        "misconception": "Targets motivation confusion: Student may focus on impact/reward rather than technical exploitability."
      },
      {
        "question_text": "The application&#39;s uptime and performance metrics",
        "misconception": "Targets operational confusion: Student may confuse operational reliability with security quality."
      },
      {
        "question_text": "The presence of a bug bounty program, suggesting a mature security team",
        "misconception": "Targets incentive confusion: Student may focus on the presence of a program rather than the underlying architectural quality."
      }
    ],
    "detailed_explanation": {
      "core_logic": "Hackers use the application&#39;s security architecture as a proxy for the overall quality of the code and its security controls. A poorly designed or inconsistent security architecture often leads to widespread vulnerabilities, making it a more attractive target for exploitation due to the higher likelihood of discovering significant flaws.",
      "distractor_analysis": "While the number of users can indicate potential impact, it doesn&#39;t directly signal the ease of exploitation. Uptime and performance are operational metrics, not security indicators. The presence of a bug bounty program might suggest a security-conscious organization, but it doesn&#39;t inherently mean the underlying architecture is robust; in fact, it might even indicate a known need for external help.",
      "analogy": "A hacker looking at an application&#39;s architecture is like a burglar assessing a house&#39;s blueprints – they&#39;re looking for structural weaknesses that could lead to multiple entry points, not just a single unlocked window."
    },
    "code_snippets": [],
    "difficulty": "advanced",
    "question_type": "analysis",
    "prerequisites": [
      "WEB_APP_SECURITY_ARCHITECTURE",
      "ATTACKER_MINDSET"
    ]
  },
  {
    "question_text": "Why do applications on a NUMA system often set an affinity mask to restrict a process to processors within a specific node?",
    "correct_answer": "To maximize performance by ensuring the process primarily accesses the faster, node-local memory.",
    "distractors": [
      {
        "question_text": "To prevent unauthorized access to memory regions in other NUMA nodes.",
        "misconception": "Targets security function confusion: Student may incorrectly attribute a security purpose to affinity masks, rather than a performance optimization."
      },
      {
        "question_text": "To balance the workload evenly across all available NUMA nodes.",
        "misconception": "Targets load balancing confusion: Student may confuse affinity (restricting to a subset) with load balancing (distributing across all)."
      },
      {
        "question_text": "Because Windows&#39; NUMA-aware scheduling algorithms do not effectively handle cross-node memory access.",
        "misconception": "Targets system capability underestimation: Student may underestimate Windows&#39; built-in NUMA optimizations."
      }
    ],
    "detailed_explanation": {
      "core_logic": "In a NUMA system, memory access is non-uniform; node-local memory is significantly faster. Applications set an affinity mask to restrict their threads to processors within a single NUMA node. This ensures that the process primarily uses the high-speed local memory associated with that node, thereby maximizing performance by reducing latency associated with accessing remote memory.",
      "distractor_analysis": "Affinity masks are for performance optimization, not security; all processors can access all memory. Setting an affinity mask restricts a process to a subset of processors, which is the opposite of balancing workload across all nodes. Windows&#39; NUMA-aware scheduling algorithms are designed to effectively handle NUMA systems and already restrict most threads to a single node, but explicit affinity can still provide further optimization for highly sensitive applications.",
      "analogy": "It&#39;s like a chef working in a large kitchen. If all their ingredients are on their immediate workstation (local memory), they can cook much faster than if they constantly have to walk to another part of the kitchen (remote memory) to get ingredients."
    },
    "code_snippets": [],
    "difficulty": "advanced",
    "question_type": "analysis",
    "prerequisites": [
      "NUMA_ARCHITECTURE",
      "PROCESS_SCHEDULING",
      "PERFORMANCE_OPTIMIZATION"
    ]
  }
]